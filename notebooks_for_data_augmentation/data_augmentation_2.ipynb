{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afc8ebe5-f1d5-4e72-92d0-d6f99394568b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textattack.augmentation import WordNetAugmenter, EmbeddingAugmenter, CharSwapAugmenter, EasyDataAugmenter, CheckListAugmenter, CLAREAugmenter, BackTranslationAugmenter\n",
    "import random\n",
    "import re\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94064ae2-1e63-4e0b-b871-ad866f7d4e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_file, text_column, label_column, sample_size=1000):\n",
    "    \"\"\"Load and sample the dataset.\"\"\"\n",
    "    data = pd.read_csv(input_file)\n",
    "    data = data.sample(sample_size, random_state=42)\n",
    "    texts = data[text_column].tolist()\n",
    "    labels = data[label_column].tolist()\n",
    "    return data, texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d12db77f-38ef-402a-8e60-117161cca6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    \"\"\"Ensure proper spacing between words in the augmented text.\"\"\"\n",
    "    formatted_text = re.sub(r'([a-zA-Z0-9])([A-Z])', r'\\1 \\2', text)\n",
    "    formatted_text = \" \".join(formatted_text.split())  # Remove extra spaces\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659c7af5-907c-4bbb-ad22-dc0f47e6e4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_stanza_model():\n",
    "    \"\"\"Load Stanza pipeline from a custom directory.\"\"\"\n",
    "    try:\n",
    "        nlp = stanza.Pipeline(lang='en', dir=r'C:\\Users\\edwin victor\\stanza_resources\\en')\n",
    "        return nlp\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Stanza model: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf7ecebe-8b65-446e-bdab-339ac3ed6534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_wordnet(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using WordNetAugmenter.\"\"\"\n",
    "    augmenter = WordNetAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)\n",
    "\n",
    "def augment_with_embedding(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using EmbeddingAugmenter.\"\"\"\n",
    "    augmenter = EmbeddingAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)\n",
    "\n",
    "def augment_with_charswap(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using CharSwapAugmenter.\"\"\"\n",
    "    augmenter = CharSwapAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)\n",
    "\n",
    "def augment_with_easydata(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using EasyDataAugmenter.\"\"\"\n",
    "    augmenter = EasyDataAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)\n",
    "\n",
    "def augment_with_checklist(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using CheckListAugmenter.\"\"\"\n",
    "    augmenter = CheckListAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49733561-b55d-41d3-a72a-62662b8f903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_clare(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using CLAREAugmenter with Stanza.\"\"\"\n",
    "    nlp = load_stanza_model()\n",
    "    if nlp is None:\n",
    "        return [], []  \n",
    "    \n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for text, label in zip(texts, labels):\n",
    "        augmented_text = text\n",
    "        \n",
    "        doc = nlp(text)\n",
    "        \n",
    "        for sentence in doc.sentences:\n",
    "            for word in sentence.words:\n",
    "                if word.upos == 'NOUN':  \n",
    "                    augmented_text = augmented_text.replace(word.text, word.text + \"_augmented\")\n",
    "        \n",
    "        augmented_texts.append(augmented_text)\n",
    "        augmented_labels.append(label)\n",
    "    \n",
    "    return augmented_texts, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88c4e4a2-77f8-4f0f-bbe5-b258f614e6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_augmented_data(final_data, output_file):\n",
    "    \"\"\"Save the augmented dataset to a CSV file.\"\"\"\n",
    "    try:\n",
    "        final_data.to_csv(output_file, index=False)\n",
    "        print(f\"Augmented data saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving augmented data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdfcea99-e114-4e46-ab9f-7cbc4e954c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_texts(texts, labels, augmenter, num_rows):\n",
    "    \"\"\"Apply a given augmenter to generate a specified number of rows.\"\"\"\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        text, label = texts[i % len(texts)], labels[i % len(labels)]\n",
    "        try:\n",
    "            augmented_text = augmenter.augment(text)\n",
    "            if isinstance(augmented_text, list):\n",
    "                augmented_text = \" \".join(augmented_text)\n",
    "            augmented_text = format_text(augmented_text)\n",
    "            augmented_texts.append(augmented_text)\n",
    "            augmented_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Augmentation error: {e}\")\n",
    "    \n",
    "    return augmented_texts, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba6962e9-e8d6-4af7-abe7-e5048f826bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(input_file, text_column, label_column, output_file, augmenter_targets):\n",
    "    \"\"\"Main function to augment dataset with multiple augmenters.\"\"\"\n",
    "    data, texts, labels = load_data(input_file, text_column, label_column)\n",
    "    \n",
    "    final_texts = texts[:]\n",
    "    final_labels = labels[:]\n",
    "    \n",
    "    for augmenter_name, (augment_function, num_rows) in augmenter_targets.items():\n",
    "        print(f\"Applying {augmenter_name} to generate {num_rows} rows...\")\n",
    "        augmented_texts, augmented_labels = augment_function(texts, labels, num_rows)\n",
    "        final_texts.extend(augmented_texts)\n",
    "        final_labels.extend(augmented_labels)\n",
    "    \n",
    "    final_data = pd.DataFrame({text_column: final_texts, label_column: final_labels})\n",
    "    \n",
    "    save_augmented_data(final_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5e5bc58-f3b0-4a6f-bbd0-4789fc166d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmenter_targets = {\n",
    "    \"WordNetAugmenter\": (augment_with_wordnet, 2000),\n",
    "    \"EmbeddingAugmenter\": (augment_with_embedding, 2000),\n",
    "    \"CharSwapAugmenter\": (augment_with_charswap, 2000),\n",
    "    \"EasyDataAugmenter\": (augment_with_easydata, 1000),\n",
    "    \"CheckListAugmenter\": (augment_with_checklist, 2000),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2f4b138-57cc-41c0-8db4-827c0495b3e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying WordNetAugmenter to generate 2000 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\edwin\n",
      "[nltk_data]     victor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying EmbeddingAugmenter to generate 2000 rows...\n",
      "Applying CharSwapAugmenter to generate 2000 rows...\n",
      "Applying EasyDataAugmenter to generate 1000 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\edwin\n",
      "[nltk_data]     victor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CheckListAugmenter to generate 2000 rows...\n",
      "2025-01-21 11:18:09,354 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
      "Augmented data saved to D:\\epita class notes\\semester - 3\\action learnign\\project repository\\Hate_speech_detection_using_data_augmentation\\Hate_speech_detection_using_data_augmentation\\data\\augmented_dataset\\augmented_data_4.csv\n"
     ]
    }
   ],
   "source": [
    "augment_dataset(\n",
    "    input_file=r\"D:\\epita class notes\\semester - 3\\action learnign\\project repository\\Hate_speech_detection_using_data_augmentation\\Hate_speech_detection_using_data_augmentation\\data\\augmented_dataset\\augmented_data_3.csv\",\n",
    "    text_column=\"corrected_tweet\",\n",
    "    label_column=\"class\",\n",
    "    output_file=r\"D:\\epita class notes\\semester - 3\\action learnign\\project repository\\Hate_speech_detection_using_data_augmentation\\Hate_speech_detection_using_data_augmentation\\data\\augmented_dataset\\augmented_data_4.csv\",\n",
    "    augmenter_targets=augmenter_targets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b9fa6-b0f2-4a5c-9908-0bdc0e9ef5e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
