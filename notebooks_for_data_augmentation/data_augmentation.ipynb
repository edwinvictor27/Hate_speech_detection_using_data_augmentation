{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "71f0f071-429e-4849-814c-8854207818db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textattack.augmentation import WordNetAugmenter\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "33725d74-c714-4125-8e08-740c553ca86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_file, text_column, label_column):\n",
    "    data = pd.read_csv(input_file)\n",
    "    data = data.sample(1000, random_state=42)  # Sample 1000 rows\n",
    "    texts = data[text_column].tolist()\n",
    "    labels = data[label_column].tolist()\n",
    "    return data, texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "221411ac-8714-43db-8a55-a9c9caf181d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_augmentation_details(original_size, target_size):\n",
    "    augmentation_factor = (target_size - original_size) // original_size  # Full rounds of augmentation\n",
    "    remaining_rows = (target_size - original_size) % original_size  # Handle remainder\n",
    "    return augmentation_factor, remaining_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "57518188-1a18-446f-a7aa-4e655d42687e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    \"\"\"Ensure proper spacing between words in the augmented text.\"\"\"\n",
    "    # Use regex to split text correctly and join with single spaces\n",
    "    formatted_text = re.sub(r'([a-zA-Z0-9])([A-Z])', r'\\1 \\2', text)  # Space between words in camelCase\n",
    "    formatted_text = \" \".join(formatted_text.split())  # Remove extra spaces if any\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7c3ef9b8-c375-41d5-a6e5-63fd30d89ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_texts(texts, labels, augmentation_factor, remaining_rows, augmenter):\n",
    "    \"\"\"Augment the texts using the selected augmenter.\"\"\"\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    # Augment each row a fixed number of times\n",
    "    for text, label in zip(texts, labels):\n",
    "        for _ in range(augmentation_factor):\n",
    "            try:\n",
    "                augmented_text = augmenter.augment(text)\n",
    "                if isinstance(augmented_text, list):\n",
    "                    augmented_text = \" \".join(augmented_text)  # Join list of words into a single string\n",
    "                augmented_text = format_text(augmented_text)  # Ensure proper spacing\n",
    "                augmented_texts.append(augmented_text)\n",
    "                augmented_labels.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Augmentation error: {e}\")\n",
    "    \n",
    "    # Handle the remaining rows\n",
    "    for i in range(remaining_rows):\n",
    "        try:\n",
    "            augmented_text = augmenter.augment(texts[i])\n",
    "            if isinstance(augmented_text, list):\n",
    "                augmented_text = \" \".join(augmented_text)  # Join list of words into a single string\n",
    "            augmented_text = format_text(augmented_text)  # Ensure proper spacing\n",
    "            augmented_texts.append(augmented_text)\n",
    "            augmented_labels.append(labels[i])\n",
    "        except Exception as e:\n",
    "            print(f\"Augmentation error: {e}\")\n",
    "    \n",
    "    return augmented_texts, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16cd57b8-4987-43a7-92cc-a1b1e59521cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_augmented_data(final_data, output_file):\n",
    "    \"\"\"Save the augmented data to a CSV file.\"\"\"\n",
    "    final_data.to_csv(output_file, index=False)\n",
    "    print(f\"Augmented dataset saved to '{output_file}'. Total rows: {len(final_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4aa56cf5-4fc7-4f0c-a920-56fa4ebabc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(input_file, text_column, label_column, output_file, target_size=1500):\n",
    "    \"\"\"Main function to augment dataset.\"\"\"\n",
    "    # Load data\n",
    "    data, texts, labels = load_data(input_file, text_column, label_column)\n",
    "    original_size = len(texts)\n",
    "    augmentation_factor, remaining_rows = calculate_augmentation_details(original_size, target_size)\n",
    "    \n",
    "    # Initialize WordNet augmenter\n",
    "    wordnet_augmenter = WordNetAugmenter()\n",
    "    \n",
    "    # Augment texts and labels\n",
    "    augmented_texts, augmented_labels = augment_texts(texts, labels, augmentation_factor, remaining_rows, wordnet_augmenter)\n",
    "    \n",
    "    # Combine original and augmented data\n",
    "    final_texts = texts + augmented_texts\n",
    "    final_labels = labels + augmented_labels\n",
    "    \n",
    "    # Create final DataFrame with augmented texts and labels\n",
    "    final_data = pd.DataFrame({text_column: final_texts, label_column: final_labels})\n",
    "    \n",
    "    # Save the final augmented dataset\n",
    "    save_augmented_data(final_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ef64a7e-6480-4852-bf2d-2ee5bf78b3b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\edwin\n",
      "[nltk_data]     victor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented dataset saved to 'D:\\epita class notes\\semester - 3\\action learnign\\project repository\\Hate_speech_detection_using_data_augmentation\\Hate_speech_detection_using_data_augmentation\\data\\augmented_dataset\\augmented_data.csv'. Total rows: 1500\n"
     ]
    }
   ],
   "source": [
    "augment_dataset(\n",
    "    input_file=r\"D:\\epita class notes\\semester - 3\\action learnign\\project repository\\Hate_speech_detection_using_data_augmentation\\Hate_speech_detection_using_data_augmentation\\data\\cleaned_dataset\\labeled_data_cleaned.csv\",  # Path to your dataset\n",
    "    text_column=\"corrected_tweet\",  # Column to augment\n",
    "    label_column=\"class\",\n",
    "    output_file=r\"D:\\epita class notes\\semester - 3\\action learnign\\project repository\\Hate_speech_detection_using_data_augmentation\\Hate_speech_detection_using_data_augmentation\\data\\augmented_dataset\\augmented_data.csv\",  # Output file for augmented dataset\n",
    "    target_size= 1500  # Desired size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5411ffb8-e0f2-49f6-afe9-66fc3b87e886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow_hub\n",
      "  Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: numpy>=1.12.0 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from tensorflow_hub) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from tensorflow_hub) (4.25.5)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from tensorflow_hub) (2.15.0)\n",
      "Downloading tensorflow_hub-0.16.1-py2.py3-none-any.whl (30 kB)\n",
      "Installing collected packages: tensorflow_hub\n",
      "Successfully installed tensorflow_hub-0.16.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83a80d5-9ec2-4d0d-bdcf-82641ff8251d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
