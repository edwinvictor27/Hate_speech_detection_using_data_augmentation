{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "8gzoL3YFDkve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FecjJMIDUsW"
      },
      "outputs": [],
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Load backtranslation model and tokenizer\n",
        "def load_translation_model(src_lang, tgt_lang):\n",
        "    model_name = f\"Helsinki-NLP/opus-mt-{src_lang}-{tgt_lang}\"\n",
        "    tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "    model = MarianMTModel.from_pretrained(model_name)\n",
        "    return model, tokenizer\n",
        "\n",
        "# Translate text in batches\n",
        "def translate_text(texts, model, tokenizer, device='cpu', batch_size=8):\n",
        "    model = model.to(device)\n",
        "    translations = []\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        outputs = model.generate(**inputs)\n",
        "        decoded = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "        translations.extend(decoded)\n",
        "    return translations\n",
        "\n",
        "# Backtranslate texts\n",
        "def backtranslate_texts(texts, device='cpu'):\n",
        "    # Load translation models\n",
        "    en_to_fr_model, en_to_fr_tokenizer = load_translation_model('en', 'fr')\n",
        "    fr_to_en_model, fr_to_en_tokenizer = load_translation_model('fr', 'en')\n",
        "\n",
        "    # Step 1: Translate to French\n",
        "    translated_to_fr = translate_text(texts, en_to_fr_model, en_to_fr_tokenizer, device)\n",
        "\n",
        "    # Step 2: Translate back to English\n",
        "    backtranslated = translate_text(translated_to_fr, fr_to_en_model, fr_to_en_tokenizer, device)\n",
        "\n",
        "    return backtranslated\n",
        "\n",
        "# Load dataset\n",
        "dataset_path = '/content/labeled_data_cleaned_whole.csv'\n",
        "data = pd.read_csv(dataset_path)\n",
        "\n",
        "# Clean missing values\n",
        "data['corrected_tweet'] = data['corrected_tweet'].fillna('')\n",
        "data['corrected_tweet'] = data['corrected_tweet'].astype(str)\n",
        "\n",
        "# Separate the hate speech class (class 0)\n",
        "class_0 = data[data['class'] == 0]\n",
        "\n",
        "# Augment class 0 with backtranslation\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "augmented_texts = backtranslate_texts(class_0['corrected_tweet'].tolist(), device)\n",
        "\n",
        "# Create a new DataFrame for augmented data\n",
        "augmented_class_0 = pd.DataFrame({\n",
        "    'corrected_tweet': augmented_texts,\n",
        "    'class': [0] * len(augmented_texts)\n",
        "})\n",
        "\n",
        "# Combine augmented data with the original dataset\n",
        "augmented_data = pd.concat([data, augmented_class_0])\n",
        "augmented_data = augmented_data.sample(frac=1, random_state=42)\n",
        "\n",
        "# Save the augmented dataset\n",
        "augmented_dataset_path = '/content/augmented_dataset.csv'\n",
        "augmented_data.to_csv(augmented_dataset_path, index=False)\n",
        "print(f\"Augmented dataset saved to {augmented_dataset_path}\")\n"
      ]
    }
  ]
}