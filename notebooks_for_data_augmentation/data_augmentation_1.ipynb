{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "343d41fa-b245-4c51-9b7a-3587e82438a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from textattack.augmentation import WordNetAugmenter, EmbeddingAugmenter, CharSwapAugmenter, EasyDataAugmenter, CheckListAugmenter, CLAREAugmenter, BackTranslationAugmenter\n",
    "from textattack.transformations import WordS\n",
    "import random\n",
    "import re\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66ab93ea-e9be-4813-845c-1e035fb4517a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "628aff84600b46c59ba00142af9b8fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded file to C:\\Users\\edwin victor\\stanza_resources\\resources.json\n",
      "Downloading default packages for language: en (English) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b51a2bef46a4cf492d2be9bd1213b5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.10.0/models/default.zip:   0%|          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded file to C:\\Users\\edwin victor\\stanza_resources\\en\\default.zip\n",
      "Finished downloading models and saved to C:\\Users\\edwin victor\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "stanza.download('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28bd140a-5efe-4f72-86cc-43408dadf938",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae5091335764cee9db83519d52ce74a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded file to C:\\Users\\edwin victor\\stanza_resources\\resources.json\n",
      "Downloading default packages for language: fr (French) ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d088f2006dbf4598b7b2f502cc827776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://huggingface.co/stanfordnlp/stanza-fr/resolve/v1.10.0/models/default.zip:   0%|          | …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded file to C:\\Users\\edwin victor\\stanza_resources\\fr\\default.zip\n",
      "Finished downloading models and saved to C:\\Users\\edwin victor\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "stanza.download('fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62afd743-f355-400e-830c-01f722694aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_file, text_column, label_column, sample_size=1000):\n",
    "    \"\"\"Load and sample the dataset.\"\"\"\n",
    "    data = pd.read_csv(input_file)\n",
    "    data = data.sample(sample_size, random_state=42)\n",
    "    texts = data[text_column].tolist()\n",
    "    labels = data[label_column].tolist()\n",
    "    return data, texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11c7693b-1154-4284-807e-f46a096297ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(text):\n",
    "    \"\"\"Ensure proper spacing between words in the augmented text.\"\"\"\n",
    "    formatted_text = re.sub(r'([a-zA-Z0-9])([A-Z])', r'\\1 \\2', text)\n",
    "    formatted_text = \" \".join(formatted_text.split())  # Remove extra spaces\n",
    "    return formatted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56df2cfe-ddb3-40f5-a2ac-c4e04d391156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_wordnet(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using WordNetAugmenter.\"\"\"\n",
    "    augmenter = WordNetAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddb55662-b05a-4bc1-8cf7-269ccbfe7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_embedding(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using EmbeddingAugmenter.\"\"\"\n",
    "    augmenter = EmbeddingAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88a12a8c-2651-498f-9b57-7aae7d7a85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_charswap(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using CharSwapAugmenter.\"\"\"\n",
    "    augmenter = CharSwapAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c977fd9d-c263-422c-a51f-4c42e313df75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_easydata(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using EasyDataAugmenter.\"\"\"\n",
    "    augmenter = EasyDataAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5fff75d-25b5-494b-9f6c-07c8cb829ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_checklist(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using CheckListAugmenter.\"\"\"\n",
    "    augmenter = CheckListAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ce4ddd4-354e-4955-899d-660d653bd2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_clare(texts, labels, num_rows):\n",
    "    \"\"\"Augment texts using CLAREAugmenter.\"\"\"\n",
    "    augmenter = CLAREAugmenter()\n",
    "    return augment_texts(texts, labels, augmenter, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8657c2c0-9504-4eb7-b47f-a1a5b8af746f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_with_back_translator(texts, labels, num_rows):\n",
    "    augmenter = BackTranslationAugmenter(from_lang='en', to_lang='fr') \n",
    "    return augment_texts(texts, labels, augmenter, num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe09367c-25b7-455c-b383-dd3c65305634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_texts(texts, labels, augmenter, num_rows):\n",
    "    \"\"\"Apply a given augmenter to generate a specified number of rows.\"\"\"\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        text, label = texts[i % len(texts)], labels[i % len(labels)]\n",
    "        try:\n",
    "            augmented_text = augmenter.augment(text)\n",
    "            if isinstance(augmented_text, list):\n",
    "                augmented_text = \" \".join(augmented_text)\n",
    "            augmented_text = format_text(augmented_text)\n",
    "            augmented_texts.append(augmented_text)\n",
    "            augmented_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Augmentation error: {e}\")\n",
    "    \n",
    "    return augmented_texts, augmented_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cc5d48ef-1cd4-4dd9-9162-f1966569951f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_texts(texts, labels, augmenter, num_rows):\n",
    "    \"\"\"Apply a given augmenter to generate a specified number of rows.\"\"\"\n",
    "    augmented_texts = []\n",
    "    augmented_labels = []\n",
    "    \n",
    "    for i in range(num_rows):\n",
    "        text, label = texts[i % len(texts)], labels[i % len(labels)]\n",
    "        try:\n",
    "            augmented_text = augmenter.augment(text)\n",
    "            # Ensure that the augmented text is not repeated\n",
    "            if isinstance(augmented_text, list):\n",
    "                augmented_text = \" \".join(augmented_text)\n",
    "            augmented_text = format_text(augmented_text)\n",
    "            \n",
    "            if augmented_text not in augmented_texts:  # Prevent repetition\n",
    "                augmented_texts.append(augmented_text)\n",
    "                augmented_labels.append(label)\n",
    "        except Exception as e:\n",
    "            print(f\"Augmentation error: {e}\")\n",
    "    \n",
    "    return augmented_texts, augmented_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad241085-6de0-4f73-ab5d-c4b348c44fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_augmented_data(final_data, output_file):\n",
    "    \"\"\"Save the augmented dataset to a CSV file.\"\"\"\n",
    "    try:\n",
    "        final_data.to_csv(output_file, index=False)\n",
    "        print(f\"Augmented data saved to {output_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving augmented data: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0522b6e0-0868-44b0-8de5-bc8a01d9f50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dataset(input_file, text_column, label_column, output_file, augmenter_targets):\n",
    "    \"\"\"Main function to augment dataset with multiple augmenters.\"\"\"\n",
    "    # Load data\n",
    "    data, texts, labels = load_data(input_file, text_column, label_column)\n",
    "    \n",
    "    final_texts = texts[:]\n",
    "    final_labels = labels[:]\n",
    "    \n",
    "    # Apply each augmenter and combine the results\n",
    "    for augmenter_name, (augment_function, num_rows) in augmenter_targets.items():\n",
    "        print(f\"Applying {augmenter_name} to generate {num_rows} rows...\")\n",
    "        augmented_texts, augmented_labels = augment_function(texts, labels, num_rows)\n",
    "        final_texts.extend(augmented_texts)\n",
    "        final_labels.extend(augmented_labels)\n",
    "    \n",
    "    # Create final DataFrame\n",
    "    final_data = pd.DataFrame({text_column: final_texts, label_column: final_labels})\n",
    "    \n",
    "    # Save the augmented dataset\n",
    "    save_augmented_data(final_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "11b8863d-a99a-47a9-9707-1b819ef568ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define augmenters and number of rows for each\n",
    "augmenter_targets = {\n",
    "    \"WordNetAugmenter\": (augment_with_wordnet, 300),\n",
    "    \"EmbeddingAugmenter\": (augment_with_embedding, 300),\n",
    "    \"CharSwapAugmenter\": (augment_with_charswap, 200),\n",
    "    \"EasyDataAugmenter\": (augment_with_easydata, 300),\n",
    "    \"CheckListAugmenter\": (augment_with_checklist, 200),\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "afe17821-1622-4894-ba4c-5849927d0ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying WordNetAugmenter to generate 300 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\edwin\n",
      "[nltk_data]     victor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying EmbeddingAugmenter to generate 300 rows...\n",
      "Applying CharSwapAugmenter to generate 200 rows...\n",
      "Applying EasyDataAugmenter to generate 300 rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\edwin\n",
      "[nltk_data]     victor\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying CheckListAugmenter to generate 200 rows...\n",
      "Augmented data saved to D:\\epita class notes\\semester - 3\\action learnign\\project repository\\Hate_speech_detection_using_data_augmentation\\Hate_speech_detection_using_data_augmentation\\data\\augmented_dataset\\augmented_data_1.1.csv\n"
     ]
    }
   ],
   "source": [
    "# Run augmentation\n",
    "augment_dataset(\n",
    "    input_file=r\"D:\\epita class notes\\semester - 3\\action learnign\\project repository\\Hate_speech_detection_using_data_augmentation\\Hate_speech_detection_using_data_augmentation\\data\\cleaned_dataset\\labeled_data_cleaned.csv\",\n",
    "    text_column=\"corrected_tweet\",\n",
    "    label_column=\"class\",\n",
    "    output_file=r\"D:\\epita class notes\\semester - 3\\action learnign\\project repository\\Hate_speech_detection_using_data_augmentation\\Hate_speech_detection_using_data_augmentation\\data\\augmented_dataset\\augmented_data_1.1.csv\",\n",
    "    augmenter_targets=augmenter_targets\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8860734-2e9a-49eb-b924-a5ad80402dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563b6cc1e7fa41b8ab4dc765d8f91ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded file to C:\\Users\\edwin victor\\stanza_resources\\resources.json\n",
      "Downloading default packages for language: en (English) ...\n",
      "File exists: C:\\Users\\edwin victor\\stanza_resources\\en\\default.zip\n",
      "Finished downloading models and saved to C:\\Users\\edwin victor\\stanza_resources\n",
      "Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aecfdf31e8e34bf98dde2e9440c938f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloaded file to C:\\Users\\edwin victor\\stanza_resources\\resources.json\n",
      "Loading these models for language: en (English):\n",
      "============================================\n",
      "| Processor    | Package                   |\n",
      "--------------------------------------------\n",
      "| tokenize     | combined                  |\n",
      "| mwt          | combined                  |\n",
      "| pos          | combined_charlm           |\n",
      "| lemma        | combined_nocharlm         |\n",
      "| constituency | ptb3-revised_charlm       |\n",
      "| depparse     | combined_charlm           |\n",
      "| sentiment    | sstplus_charlm            |\n",
      "| ner          | ontonotes-ww-multi_charlm |\n",
      "============================================\n",
      "\n",
      "Using device: cpu\n",
      "Loading: tokenize\n",
      "Loading: mwt\n",
      "Loading: pos\n",
      "Loading: lemma\n",
      "Loading: constituency\n",
      "Loading: depparse\n",
      "Loading: sentiment\n",
      "Loading: ner\n",
      "Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: This, upos: PRON\n",
      "word: is, upos: AUX\n",
      "word: a, upos: DET\n",
      "word: test, upos: NOUN\n",
      "word: sentence, upos: NOUN\n",
      "word: ., upos: PUNCT\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "stanza.download('en')  # Ensure the model is downloaded\n",
    "\n",
    "nlp = stanza.Pipeline('en')\n",
    "doc = nlp(\"This is a test sentence.\")\n",
    "for sentence in doc.sentences:\n",
    "    for word in sentence.words:\n",
    "        print(f\"word: {word.text}, upos: {word.upos}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ab7635e-a91c-4169-bb10-c61aa99ba3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.10.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting emoji (from stanza)\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: numpy in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from stanza) (1.26.4)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from stanza) (4.25.5)\n",
      "Requirement already satisfied: requests in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from stanza) (2.32.3)\n",
      "Requirement already satisfied: networkx in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from stanza) (3.4.2)\n",
      "Requirement already satisfied: tomli in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from stanza) (2.2.1)\n",
      "Requirement already satisfied: torch>=1.3.0 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from stanza) (2.5.1)\n",
      "Requirement already satisfied: tqdm in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from stanza) (4.67.1)\n",
      "Requirement already satisfied: filelock in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from torch>=1.3.0->stanza) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
      "Requirement already satisfied: jinja2 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.5)\n",
      "Requirement already satisfied: fsspec in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from torch>=1.3.0->stanza) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from requests->stanza) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from requests->stanza) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from requests->stanza) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from requests->stanza) (2024.12.14)\n",
      "Requirement already satisfied: colorama in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\miniconda\\miniconda\\envs\\hate_speech\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
      "Downloading stanza-1.10.1-py3-none-any.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.3/1.1 MB ? eta -:--:--\n",
      "   ------------------ --------------------- 0.5/1.1 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.1/1.1 MB 1.8 MB/s eta 0:00:00\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 586.9/586.9 kB 3.0 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji, stanza\n",
      "Successfully installed emoji-2.14.0 stanza-1.10.1\n"
     ]
    }
   ],
   "source": [
    "!pip install stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd551166-f461-49a6-8769-ad22a970e65a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
