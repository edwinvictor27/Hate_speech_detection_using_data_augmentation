{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aac9bcd5-9347-4bb3-bec3-012141b55454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\asus\\anaconda3\\lib\\site-packages (3.8.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.0.11)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (8.3.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.5.0)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (2.32.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (1.10.19)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (69.5.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from spacy) (24.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Requirement already satisfied: blis<1.1.0,>=1.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (1.0.2)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from thinc<8.4.0,>=8.3.0->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\roaming\\python\\python312\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\asus\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n"
     ]
    }
   ],
   "source": [
    "pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dce9042c-082e-4f79-8351-350383653893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.0/12.8 MB 495.5 kB/s eta 0:00:26\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "     ---------------------------------------- 0.1/12.8 MB 1.4 MB/s eta 0:00:09\n",
      "      --------------------------------------- 0.2/12.8 MB 1.1 MB/s eta 0:00:11\n",
      "     - -------------------------------------- 0.4/12.8 MB 1.8 MB/s eta 0:00:08\n",
      "     - -------------------------------------- 0.6/12.8 MB 2.1 MB/s eta 0:00:06\n",
      "     -- ------------------------------------- 0.8/12.8 MB 2.3 MB/s eta 0:00:06\n",
      "     --- ------------------------------------ 1.0/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     --- ------------------------------------ 1.2/12.8 MB 2.7 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.7 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.3/12.8 MB 2.7 MB/s eta 0:00:05\n",
      "     ---- ----------------------------------- 1.5/12.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.7/12.8 MB 2.8 MB/s eta 0:00:05\n",
      "     ----- ---------------------------------- 1.9/12.8 MB 2.9 MB/s eta 0:00:04\n",
      "     ------ --------------------------------- 2.1/12.8 MB 3.0 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.3/12.8 MB 3.1 MB/s eta 0:00:04\n",
      "     ------- -------------------------------- 2.5/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "     -------- ------------------------------- 2.8/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     --------- ------------------------------ 3.0/12.8 MB 3.4 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.2/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ---------- ----------------------------- 3.4/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.6/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ----------- ---------------------------- 3.8/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     ------------ --------------------------- 4.0/12.8 MB 3.6 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.2/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     ------------- -------------------------- 4.4/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     -------------- ------------------------- 4.6/12.8 MB 3.7 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 4.8/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     --------------- ------------------------ 5.0/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.2/12.8 MB 3.8 MB/s eta 0:00:03\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.3/12.8 MB 3.8 MB/s eta 0:00:02\n",
      "     ---------------- ----------------------- 5.4/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.5/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 3.5 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.6/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ----------------- ---------------------- 5.7/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 5.9/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------------ --------------------- 6.1/12.8 MB 3.3 MB/s eta 0:00:03\n",
      "     ------------------- -------------------- 6.3/12.8 MB 3.3 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.5/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     -------------------- ------------------- 6.7/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     --------------------- ------------------ 6.9/12.8 MB 3.4 MB/s eta 0:00:02\n",
      "     ---------------------- ----------------- 7.1/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.4/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "     ----------------------- ---------------- 7.6/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 7.8/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "     ------------------------ --------------- 8.0/12.8 MB 3.5 MB/s eta 0:00:02\n",
      "     ------------------------- -------------- 8.2/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.4/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     -------------------------- ------------- 8.6/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     --------------------------- ------------ 8.8/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.0/12.8 MB 3.6 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ----------------------------- ---------- 9.5/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 9.7/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.0/12.8 MB 3.7 MB/s eta 0:00:01\n",
      "     ------------------------------- -------- 10.2/12.8 MB 3.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.4/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 10.7/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 11.0/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.2/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 11.4/12.8 MB 4.0 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.6/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 11.8/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 12.0/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.2/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.4/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.6/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 4.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 4.0 MB/s eta 0:00:00\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7604c1bf-93bb-45a3-b351-6bdd4b40317e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to: D:/SEM 3/Action Learning/cleaned_labeled_data2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"D:/SEM 3/Action Learning/labeled_data.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "dataset.drop(columns=['Unnamed: 0', 'count'], inplace=True)\n",
    "\n",
    "# Function to clean text using spaCy\n",
    "def clean_text_spacy(text):\n",
    "    # Remove URLs, mentions, hashtags, and special characters\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)    # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)    # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize and remove stopwords\n",
    "    cleaned_text = ' '.join(\n",
    "        token.lemma_ for token in doc if not token.is_stop and not token.is_punct\n",
    "    )\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply cleaning to the 'tweet' column\n",
    "dataset['tweet_cleaned'] = dataset['tweet'].apply(clean_text_spacy)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_file_path = \"D:/SEM 3/Action Learning/cleaned_labeled_data2.csv\"\n",
    "dataset.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {cleaned_file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07d91e30-117a-4444-9a7a-65ba0eff9ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to: D:/SEM 3/Action Learning/cleaned_labeled_data_output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"D:/SEM 3/Action Learning/labeled_data.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "dataset.drop(columns=['Unnamed: 0', 'count'], inplace=True)\n",
    "\n",
    "# Function to clean text using spaCy\n",
    "def clean_text_spacy(text):\n",
    "    # Remove URLs, mentions, hashtags, and special characters\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)    # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)    # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    \n",
    "    # Keep the original case\n",
    "    text = text.strip()\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize and keep meaningful tokens\n",
    "    cleaned_text = ' '.join(\n",
    "        token.lemma_ for token in doc if not token.is_stop and not token.is_punct\n",
    "    )\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Apply cleaning to the 'tweet' column\n",
    "dataset['tweet_cleaned'] = dataset['tweet'].apply(clean_text_spacy)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_file_path = \"D:/SEM 3/Action Learning/cleaned_labeled_data_output.csv\"\n",
    "dataset.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cbc6dded-e7b6-4e0f-add9-9c4d81066c70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to: D:/SEM 3/Action Learning/cleaned_labeled_data_output2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"D:/SEM 3/Action Learning/labeled_data.csv\"\n",
    "dataset = pd.read_csv(file_path)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "dataset.drop(columns=['Unnamed: 0', 'count'], inplace=True)\n",
    "\n",
    "# Function to clean text using spaCy\n",
    "def clean_text_spacy(text):\n",
    "    # Remove \"RT\", URLs, mentions, hashtags, and special characters\n",
    "    text = re.sub(r'\\bRT\\b', '', text)  # Remove \"RT\"\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)    # Remove mentions\n",
    "    text = re.sub(r'#\\w+', '', text)    # Remove hashtags\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)  # Remove special characters\n",
    "    \n",
    "    # Strip leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "\n",
    "    # Process text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Lemmatize and keep meaningful tokens\n",
    "    cleaned_text = ' '.join(\n",
    "        token.lemma_ for token in doc if not token.is_stop and not token.is_punct\n",
    "    )\n",
    "    \n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# Apply cleaning to the 'tweet' column\n",
    "dataset['tweet_cleaned'] = dataset['tweet'].apply(clean_text_spacy)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_file_path = \"D:/SEM 3/Action Learning/cleaned_labeled_data_output2.csv\"\n",
    "dataset.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"Cleaned dataset saved to: {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e103adc8-a494-4157-a0a5-6f1f6b2f094f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
